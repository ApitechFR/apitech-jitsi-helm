### Valeurs par défaut pour le chart jitsi-meet
### Veiller à bien vérifier l'ensemble des paramètres.
### La plupart des valeurs qui ne sont pas accompagnées d'un commentaire en français devraient rester inchangées.
### Les valeurs à modifier contiennent par "CHANGEME" ou "APITECH".

### Remarque supplémentaire : Définir les mots de passe xmpp (prosody.xmpp.password, jicofo.xmpp.password, jvb.xmpp.password, jigasi.xmpp.password, jibri.xmpp.password et jibri.recorder.password) règle un bug qui force à redémarrer tous les composants après un upgrade.

global:
  # Set your cluster's DNS domain here.
  # "cluster.local" should work for most environments.
  # Set to "" to disable the use of FQDNs (default in older chart versions).
  clusterDomain: cluster.local
  podLabels: {}
  podAnnotations: {}
  releaseSecretsOverride:
    enabled: false
    #Support environment variables from pre-created secrets, such as 1Password operator
    #extraEnvFrom:
    #  - secretRef:
    #      name: '{{ include "prosody.fullname" . }}-overrides'
    #      optional: true

### APITECH : Configuration de l'authentification
### Cette syntaxe permet de définir un bloc réutilisable à plusieurs endroits
### Nécessaire pour le prosody : il utilise un .Values différent, dont la racine est .Values.prosody
authConfig: &authConfig
  enableAuth: false
  # Optionnel
  # "internal", "jwt", "hybrid_matrix_token"
  # authType: "internal"
  jwt:
    appId: "app_id_jwt"
    secret: "secret_jwt"
  enableGuests: true

<<: *authConfig

### APITECH : Secret Kubernetes pour l'authentification aux regisitries privées
imagePullSecrets: []
nameOverride: ""
fullnameOverride: ""

# Where Jitsi Web UI is made available
# such as jitsi.example.com
### APITECH : Cette URL n'est PAS celle de l'overlay mais celle du Jitsi SANS OVERLAY.
## ATTENTION : Cette URL __doit__ être préfixée de "https://", sinon le Jibri ne marchera pas.
publicURL: "https://CHANGEME.example.com"

tz: Europe/Paris

image:
  pullPolicy: IfNotPresent

## WebSocket configuration:
#
#  Both Colibri and XMPP WebSockets are disabled by default,
#  since some LoadBalancer / Reverse Proxy setups can't pass
#  WebSocket connections properly, which might result in breakage
#  for some clients.
#
#  Enable both Colibri and XMPP WebSockets to replicate the current
#  upstream `meet.jit.si` setup. Keep both disabled to replicate
#  older setups which might be more compatible in some cases.
websockets:
  ## Colibri (JVB signalling):
  colibri:
    enabled: true
  ## XMPP (Prosody signalling):
  xmpp:
    enabled: true

web:
  replicaCount: 1
  image:
    repository: jitsi/web

  ## Override the image-provided configuration files:
  #  See https://github.com/jitsi/docker-jitsi-meet/tree/master/web/rootfs
  custom:
    ### APITECH : Possibilité d'utiliser un configmap externe pour override le fichier external_api.js
    ##  Veiller à bien le créer avant déployer le chart.
    jaliosExternalApiConfigmap: "" # ex: "my-external-api-configmap"
    contInit:
      _10_config: ""
    defaults:
      _default: ""
      _ffdhe2048_txt: ""
      _interface_config_js: ""
      _meet_conf: ""
      _nginx_conf: ""
      _settings_config_js: ""
      _ssl_conf: ""
      _system_config_js: ""
    configs:
      _custom_interface_config_js: ""
      _custom_config_js: ""

  extraEnvs: {}
  service:
    type: ClusterIP
    port: 80
    externalIPs: []

  ## APITECH : Ingress pour exposer le Jitsi SANS OVERLAY.
  ingress:
    enabled: true
    ingressClassName: "nginx"
    annotations:
      ## APITECH : Annotations génriques, ne devraient pas être modifiées
      nginx.org/proxy-read-timeout: "3600" # Nécessaire pour les WebSockets
      nginx.org/proxy-send-timeout: "3600" # Nécessaire pour les WebSockets
      nginx.org/server-snippet: |
        proxy_set_header Upgrade $http_upgrade;
        proxy_set_header Connection "Upgrade";
      acme.cert-manager.io/http01-edit-in-place: "true"
      ## APITECH : Ici, bien spécifier le cluster-issuer
      cert-manager.io/cluster-issuer: CHANGEME
    hosts:
    - host: CHANGEME.example.com
      paths:
        - /
    tls:
     - secretName: CHANGEME-tls
       hosts:
         - CHANGEME.example.com

  # Useful for ingresses that don't support http-to-https redirect by themself, (namely: GKE),
  httpRedirect: false

  # When tls-termination by the ingress is not wanted, enable this and set web.service.type=Loadbalancer
  httpsEnabled: false

  ## APITECH : Désactivation de l'enregistrement local.
  ## Laisser à true sauf contre-indication.
  disableLocalRecording: true
  ## Laisser aussi à true sauf contre-indication.
  enableLocalRecordingNotifyAllParticipant: true

  ## Resolver IP for nginx.
  #
  #  Starting with version `stable-8044`, the web container can
  #  auto-detect the nameserver from /etc/resolv.conf.
  #  Use this option if you want to override the nameserver IP.
  #
  # resolverIP: 10.43.0.10

  livenessProbe:
    httpGet:
      path: /
      port: 80
  readinessProbe:
    httpGet:
      path: /
      port: 80

  podLabels: {}
  podAnnotations: {}
  podSecurityContext: {}
    # fsGroup: 2000

  securityContext: {}
    # capabilities:
    #   drop:
    #   - ALL
    # readOnlyRootFilesystem: true
    # runAsNonRoot: true
    # runAsUser: 1000

  resources: {}
    # We usually recommend not to specify default resources and to leave this as a conscious
    # choice for the user. This also increases chances charts run on environments with little
    # resources, such as Minikube. If you do want to specify resources, uncomment the following
    # lines, adjust them as necessary, and remove the curly braces after 'resources:'.
    # limits:
    #   cpu: 100m
    #   memory: 128Mi
    # requests:
    #   cpu: 100m
    #   memory: 128Mi

  nodeSelector: {}

  tolerations: []

  affinity: {}

  ## APITECH : Paramètres de l'overlay Joona
  #  Image docker à build à partir du dépôt.
overlay:
  enabled: true
  ## APITECH : Version de l'overlay Joona.
  mode: "v1" # v1, v3
  # Exemple: image: "example-registry/jitsi-web-overlay"
  v1:
    image:
      replicas: 1
      repository: "CHANGEME"
      tag: "CHANGEME"
      pullPolicy: IfNotPresent
    extraVolumeMounts: []
    ## APITECH : Volume partagé pour l'overlay.
    #  Doit être ReadWriteMany pour permettre à tous les pods web d'accéder
    sharedMeetStorageSize: 1Gi
    ## APITECH : Utilisation d'une storageClass single-replica préférable ici, pas de données critiques.
    sharedMeetStorageClass: "CHANGEME" # ex: longhorn-single-no-retain
    service:
      type: ClusterIP
      port: 3000
      externalIPs: []
    ## APITECH : Le proxy prend le rôle du reverse proxy présent dans les VMs jitsi-web.
    # Indissociabe de l'overlay
    proxy:
      replicas: 2
      image:
        repository: nginx
        tag: stable
        pullPolicy: IfNotPresent
      service:
        type: ClusterIP
        port: 80
        externalIPs: []
      extraVolumeMounts: []
      extraVolumes: []
    ### APITECH: Gestion des assets
    ##  Soit on utilise un configmap existant => remplir la variable
    ##  Soit on utilise les assets par défaut => variable vide.
    assetsConfigmap: "jitsi-web-overlay-assets"

  ## APITECH : Ingress pour exposer l'overlay Joona.
  #  Doit être différent de celui du Jitsi sans overlay.
  ingress:
    enabled: true
    ingressClassName: "nginx"
    annotations:
      ## APITECH : Annotations génriques, ne devraient pas être modifiées
      nginx.org/proxy-read-timeout: "3600"
      nginx.org/proxy-send-timeout: "3600"
      acme.cert-manager.io/http01-edit-in-place: "true"
    ## APITECH : Ici, bien spécifier le cluster-issuer
      cert-manager.io/cluster-issuer: letsencrypt-prod
    hosts:
    - host: CHANGEME.example.com
      paths:
        - /
    tls:
     - secretName: CHANGEME-tls
       hosts:
         - CHANGEME.example.com
  database:
    useExternalDatabase: false
    externalDatabase:
      host: ""
      port: 3306
      user: jitsimeet
      password: ""

    image:
      repository: mariadb
      tag: 12.0
      pullPolicy: IfNotPresent

    storage:
      ## APITECH : Le stockage doit être ReadWriteMany pour permettre les backups.
      accessMode: "ReadWriteMany"
      size: 5Gi
      storageClass: "CHANGEME" 
    
    auth:
      user: jitsi
      database: jitsi
      password:
      rootPassword:


jicofo:
  replicaCount: 1
  image:
    repository: jitsi/jicofo

  ## Override the image-provided configuration files:
  #  See https://github.com/jitsi/docker-jitsi-meet/tree/master/jicofo/rootfs
  custom:
    contInit:
      _10_config: ""
    defaults:
      _jicofo_conf: ""
      _logging_properties: ""

  xmpp:
    password:
    componentSecret:

  livenessProbe:
    tcpSocket:
      port: 8888

  readinessProbe:
    tcpSocket:
      port: 8888

  podLabels: {}
  podAnnotations: {}
  podSecurityContext: {}
  securityContext: {}
  resources: {}
  nodeSelector: {}
  tolerations: []
  affinity: {}
  extraEnvs: {}

  ## APITECH : Paramètres de metrics pour Jicofo.
  # Nécessaire pour le dashboard Grafana.
  metrics:
    enabled: true
    image:
      repository: prayagsingh/prometheus-jicofo-exporter
      tag: "1.3.2"
    resources: {}

jvb:
  replicaCount: 1
  image:
    repository: jitsi/jvb

  xmpp:
    user: jvb
    password:

  ## APITECH : Le fonctionnement des JVBs est assez spécial. Ici, vous devez __impérativement__ 
  # spécifier autant d'IP publiques que de JVBs déployés (cf. jvb.replicaCount).
  publicIPs:
    - 1.2.3.4 # CHANGEME
    - 123.45.67.89 # CHANGEME

  # APITECH : Laisser cette valeur à false, car nous n'utilisons pas de NodePort
  useNodeIP: false
  ## Use a STUN server to help some users punch through some
  #  especially nasty NAT setups. Usually makes sense for P2P calls.
  stunServers: 'meet-jit-si-turnrelay.jitsi.net:443'
  extraVolumeMounts: []
  extraVolumes: []
  ## Try to use the hostPort feature:
  #  (might not be supported by some clouds or CNI engines)
  # useHostPort: true
  ## Use host's network namespace:
  #  (not recommended, but might help for some cases)
  # useHostNetwork: false
  ## UDP transport port:
  # UDPPort: 10000
  ## Use a pre-defined external port for NodePort or LoadBalancer service,
  #  if needed. Will allocate a random port from allowed range if unset.
  #  (Default NodePort range for K8s is 30000-32767)
  # nodePort: 10000
  # nodeportPrefix: 303

  service:
    enabled: true
    type: LoadBalancer
    externalTrafficPolicy: Cluster
    udpPort: 10000

    ## APITECH : Ces valeurs servent à construire les IPs LoadBalancer (internes au cluster!) des JVBs.
    # Veiller à ne pas écraser les IPs d'autres services LoadBalancer.
    # externalIPsPrefix est le préfixe des IPs (trois premiers octets),
    # et firstExternalIPLastOctet est le dernier octet de la première IP.
    #  Exemple: externalIPsPrefix: 172.16.0, firstExternalIPLastOctet: 1
    #  donnera les IPs suivantes:
    #  - 172.16.0.1
    #  - 172.16.0.2
    # etc.
    externalIPsPrefix: 172.16.0 # CHANGEME
    firstExternalIPLastOctet: 1

    ## If type is set to LoadBalancer and the cluster is dual stack, ipFamilyPolicy can be set to enable dual stack
    # addressing for the service.
    # ipFamilyPolicy: PreferDualStack
    ## Annotations to be added to the service (if LoadBalancer is used)
    #  An example below is needed for DigitalOcean managed k8s setups
    #  with a LoadBalancer service, so that DO's external LB can perform
    #  health checks on JVB.
    annotations: {}
      # service.beta.kubernetes.io/do-loadbalancer-healthcheck-port: "8080"
      # service.beta.kubernetes.io/do-loadbalancer-healthcheck-protocol: "tcp"
    ## Add extra ports to the service.
    #  An example below is needed for DigitalOcean managed k8s setups.
    extraPorts: []
      # - name: http-healthcheck
      #   port: 8080
      #   protocol: TCP

  breweryMuc: jvbbrewery

  livenessProbe:
    httpGet:
      path: /about/health
      port: 8080
  readinessProbe:
    httpGet:
      path: /about/health
      port: 8080

  podLabels: {}
  podAnnotations: {}
  podSecurityContext: {}
  securityContext: {}
  ## APITECH : Ressources du JVB.
  #  Valeurs ajustables selon le besoin.
  resources:
      requests:
        cpu: 3000m
        memory: 3000Mi
      limits:
        cpu: 3500m
        memory: 3500Mi
  nodeSelector: {}
  tolerations: []
  affinity: {}
  extraEnvs: {}

  ## APITECH : Paramètres de metrics pour JVB.
  # Nécessaire pour le dashboard Grafana.
  metrics:
    enabled: true
    defaultJitsiMeetExporter:
      image:
        repository: docker.io/systemli/prometheus-jitsi-meet-exporter
        tag: 1.2.3
        pullPolicy: IfNotPresent

      resources:
        requests:
          cpu: 10m
          memory: 16Mi
        limits:
          cpu: 20m
          memory: 32Mi
    ### APITECH : Changer l'image de notre exporter custom.
    apitechPrometheusExporter:
      image:
        ### APITECH : En cas d'utilisation d'une registry privée, veiller à configurer imagePullSecrets au niveau global.
        repository: registry.git.joona.fr/apitech/apitech-prometheus-jvb-exporter
        tag: latest
        pullPolicy: Always

      resources:
        requests:
          cpu: 500m
          memory: 500Mi
        limits:
          cpu: 1000m
          memory: 1Gi

    prometheusAnnotations: true
    serviceMonitor:
      enabled: true
      selector:
        release: prometheus-operator
      interval: 10s
      # honorLabels: false

    grafanaDashboards:
      enabled: true
      labels:
        grafana_dashboard: "1"
      annotations: {}

## APITECH : Encore moins testé, et je suis quasi sûr que ça cassera.
octo:
  enabled: false


jigasi:
  ## Enabling Jigasi will allow regular SIP clients to join Jitsi meetings
  ## or nearly real-time transcription.
  enabled: false

  ## Use external Jigasi installation.
  ## This setting skips the creation of Jigasi Deployment altogether,
  ## instead creating just the config secret and enabling services.
  ## Defaults to disabled (use bundled Jigasi).
  useExternalJigasi: false

  replicaCount: 1
  image:
    repository: jitsi/jigasi

  breweryMuc: jigasibrewery

  ## jigasi XMPP user credentials:
  xmpp:
    user: jigasi
    password:

  ## APITECH : Avec la version actuelle de Jigasi, les probes sont cassées (les routes sont en écoute sur le loopback).
  # TODO : Remettre les probes une fois le problème corrigé dans l'image officielle.
  livenessProbe: null
  readinessProbe: null
  # livenessProbe:
  #   httpGet:
  #     path: /about/health
  #     host: 127.0.0.1
  #     port: 8788
  #     httpHeaders:
  #       - name: Accept
  #         value: application/json
  # readinessProbe:
  #   httpGet:
  #     path: /about/health
  #     host: 127.0.0.1
  #     port: 8788
  #     httpHeaders:
  #       - name: Accept
  #         value: application/json

  podLabels: {}
  podAnnotations: {}
  podSecurityContext: {}
  securityContext: {}
  resources: {}
  nodeSelector: {}
  tolerations: []
  affinity: {}
  extraEnvs: {}

## APITECH : Paramètres de Jibri.
jibri:
  ## Enabling Jibri will allow users to record
  ## and/or stream their meetings (e.g. to YouTube).
  enabled: false

  ## Use external Jibri installation.
  ## This setting skips the creation of Jibri Deployment altogether,
  ## instead creating just the config secret
  ## and enabling recording/streaming services.
  ## Defaults to disabled (use bundled Jibri).
  useExternalJibri: false

  ## Enable single-use mode for Jibri.
  ## With this setting enabled, every Jibri instance
  ## will become "expired" after being used once (successfully or not)
  ## and cleaned up (restarted) by Kubernetes.
  ##
  ## Note that detecting expired Jibri, restarting and registering it
  ## takes some time, so you'll have to make sure you have enough
  ## instances at your disposal.
  ## You might also want to make LivenessProbe fail faster.
  singleUseMode: false

  ## Enable recording service.
  ## Set this to true/false to enable/disable local recordings.
  ## Defaults to enabled (allow local recordings).
  recording: false

  ## Enable livestreaming service.
  ## Set this to true/false to enable/disable live streams.
  ## Defaults to disabled (livestreaming is forbidden).
  livestreaming: false

  ## Enable multiple Jibri instances.
  ## If enabled (i.e. set to 2 or more), each Jibri instance
  ## will get an ID assigned to it, based on pod name.
  ## Multiple replicas are recommended for single-use mode.
  replicaCount: 1

  ## Enable persistent storage for local recordings.
  ## If disabled, jibri pod will use a transient
  ## emptyDir-backed storage instead.
  persistence:
    enabled: false
    size: 4Gi
    ## Set this to existing PVC name if you have one.
    existingClaim:
    storageClassName:

  shm:
    ## Set to true to enable "/dev/shm" mount.
    ## May be required by built-in Chromium.
    enabled: true
    ## If "true", will use host's shared memory dir,
    ## and if "false" — an emptyDir mount.
    # useHost: false
    # size: 2Gi

  ## Configure the update strategy for Jibri deployment.
  ## This may be useful depending on your persistence settings,
  ## e.g. when you use ReadWriteOnce PVCs.
  ## Default strategy is "RollingUpdate", which keeps
  ## the old instances up until the new ones are ready.
  # strategy:
  #   type: RollingUpdate

  image:
    repository: jitsi/jibri

  podLabels: {}
  podAnnotations: {}
  resources: {}

  breweryMuc: jibribrewery
  timeout: 90

  ## jibri XMPP user credentials:
  xmpp:
    user: jibri
    password:

  ## recorder XMPP user credentials:
  recorder:
    user: recorder
    password:

  livenessProbe:
    initialDelaySeconds: 5
    periodSeconds: 5
    failureThreshold: 3
    exec:
      command:
        - /bin/bash
        - "-c"
        - >-
          curl -sq localhost:2222/jibri/api/v1.0/health
          | jq '"\(.status.health.healthStatus) \(.status.busyStatus)"'
          | grep -qP 'HEALTHY (IDLE|BUSY)'

  readinessProbe:
    initialDelaySeconds: 5
    periodSeconds: 5
    failureThreshold: 3
    exec:
      command:
        - /bin/bash
        - "-c"
        - >-
          curl -sq localhost:2222/jibri/api/v1.0/health
          | jq '"\(.status.health.healthStatus) \(.status.busyStatus)"'
          | grep -qP 'HEALTHY (IDLE|BUSY)'

  nodeSelector: {}
  tolerations: []
  affinity: {}
  extraEnvs: {}

  ## Override the image-provided configuration files:
  #  See https://github.com/jitsi/docker-jitsi-meet/tree/master/jibri/rootfs
  custom:
    contInit:
      _10_config: ""
    defaults:
      _autoscaler_sidecar_config: ""
      _jibri_conf: ""
      _logging_properties: ""
      _xorg_video_dummy_conf: ""
    other:
      _finalize_sh: ""

serviceAccount:
  # Specifies whether a service account should be created
  create: true
  # Annotations to add to the service account
  annotations: {}
  # The name of the service account to use.
  # If not set and create is true, a name is generated using the fullname template
  name:

## APITECH : Le serveur TURN est utilisé pour les appels P2P et les appels avec des clients SIP.
turn:
  enabled: true
  image:
    repository: coturn/coturn
    tag: latest
    pullPolicy: IfNotPresent

  externalIP: "1.2.3.4" # CHANGEME
  serverName: "meet.jitsi" # CHANGEME
  
  service:
    port: 3478
    tlsPort: 5349
    type: LoadBalancer
    externalTrafficPolicy: Cluster
    annotations: {}
    externalIP: "CHANGEME"

  xmpp:
    password:

  ssl:
    ## APITECH : Configuration des certificats TLS pour le serveur TURN.
    #  S'il faut générer les certificats TLS pour le serveur TURN, définir à true.
    #  Si false, le chart s'attend à ce que les certificats soient présents dans le secret spécifié ci-dessous.
    #  Dans ce cas, le secret doit contenir les clés 'tls.crt' et 'tls.key'.
    generate: true
    secretName: jitsi-turn-tls
    issuer: CHANGEME


  ## APITECH : Le serveur TURN est configuré pour utiliser les IPs des JVBs.
  #  Il est donc important de veiller à ce que les JVBs soient déployés avant le TURN.

xmpp:
  domain: meet.jitsi
  authDomain:
  mucDomain:
  internalMucDomain:
  guestDomain:
  hiddenDomain:

extraCommonEnvs: {}

prosody:
  <<: *authConfig
  enabled: true
  useExternalProsody: false
  turn:
    enabled: true
    host: CHANGEME.example.com
    port: 443
  server:
  extraEnvFrom:
  - secretRef:
      name: '{{ include "prosody.fullname" . }}-jibri'
  - secretRef:
      name: '{{ include "prosody.fullname" . }}-jicofo'
  - secretRef:
      name: '{{ include "prosody.fullname" . }}-jigasi'
  - secretRef:
      name: '{{ include "prosody.fullname" . }}-jvb'
  - secretRef:
      name: '{{ include "prosody.fullname" . }}-turn'
  - configMapRef:
      name: '{{ include "prosody.fullname" . }}-common'
  image:
    repository: jitsi/prosody
    tag: stable-10314
  persistence:
    enabled: true
    size: 500Mi
    ## APITECH : Le stockage doit être ReadWriteMany pour permettre les backups.
    accessMode: "ReadWriteMany"
    storageClass: "longhorn-single-no-retain"
  ## APITECH : Désactivation du module "end_conference".
  #  À laisser désactivé sauf contre-indication.
  enableEndConference: false

  # service:
  #   ports:
  # If Prosody c2s in needed on private net outside the cluster
  #     xmppc2snodePort: 30522

  ## Override the image-provided configuration files:
  #  See https://github.com/jitsi/docker-jitsi-meet/tree/master/prosody/rootfs
  custom:
    contInit:
      _10_config: ""
    defaults:
      _prosody_cfg_lua: ""
      _saslauthd_conf: ""
      _jitsi_meet_cfg_lua: ""

  extraVolumes: []
    # - name: prosody-modules
    #   configMap:
    #     name: prosody-modules

  extraVolumeMounts: []
    # - name: prosody-modules
    #   subPath: mod_measure_client_presence.lua
    #   mountPath: /prosody-plugins-custom/mod_measure_client_presence.lua

## APITECH : Paramètres pour le dashboard Grafana et la base de données Prometheus.
#  Nécessaire pour monitorer les JVBs et Jicofo.
# Nécessite l'opérateur prometheus-operator dans le cluster.
dashboard:
  enabled: true

  prometheus:
    retention: "7d"
    storageClassName: CHANGEME
    storage:
      accessMode: "ReadWriteMany"
      size: CHANGEME

  grafana:
    enabled: true
    adminUser: CHANGEME
    adminPassword: CHANGEME
    image:
      tag: "main"

    storage:
      size: 5Gi
      accessMode: ReadWriteMany
      storageClassName: CHANGEME

    ingress:
      enabled: true
      ingressClassName: "nginx"
      annotations:
        cert-manager.io/cluster-issuer: letsencrypt-prod
        acme.cert-manager.io/http01-edit-in-place: "true"
      hosts:
        - host: CHANGEME.example.com
          paths:
            - /
      tls:
        - secretName: CHANGEME-tls
          hosts:
            - CHANGEME.example.com